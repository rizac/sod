'''
Datasets (hdf files) module with IO operations, classes definition and more

Created on 1 Nov 2019

@author: riccardo
'''
import sys
from os.path import (splitext, dirname, join, basename, isfile, isdir, isabs,
                     abspath)
from io import StringIO
from contextlib import contextmanager
import numpy as np
import pandas as pd

from sod.core import pdconcat, odict
from sod.core.paths import DATASETS_DIR

#####################
# CLASSES DEFINITIONS
#####################

# FOR ANY NEW DATASET:
# 1. IMPLEMENT THE YAML, AND PY, LAUNCH S2S AND SAVE THE DATAFRAME WITH A
#    NEW UNIQUE NAME <NAME>.HDF
# 2. IF IT HAS DIFFERENT UNIQUE COLUMNS THAN THE PREVIOUSLY DEFINED DATASETS,
#    ADDS THE UNIQUE COLUMNS BELOW (THIS WILL BE USEFUL TO GET VIA SCRIPTS
#    WHICH SEGMENT CORRESPOND TO A PREDICTION IN A PREDICTION DATAFRAME)
# 3. IF IT HAS DIFFERENT CLASSES DEFINITIONS, ADD THEM TO _CLASSES AND THEN
#    DEFINE THE RELATIVE if BRANCH IN `classes_of` (THIS WILL BE USEFUL WHEN
#    CORRECTLY DISPLAYING CONFUSION MATRICES)

# column names definition
ID_COL = 'id'  # every read operation will convert 'Segment.db.id' into this
OUTLIER_COL = 'outlier'  # MUST be defined in all datasets
MODIFIED_COL = 'modified'  # defined in pgapgv.hdf and oneminutewindows.hdf
SUBCLASS_COL = 'subclass'  # defined in magnitudeenergy.hdf
WINDOW_TYPE_COL = 'window_type'  # defined in oneminiutewindows.hdf

# defining unique id columns: in evaluation.predict, all dataframe
# passed will save the values of the following columns (those present in
# the dataframe). Also look at keep_cols in the same module:
UNIQUE_ID_COLUMNS = [ID_COL, OUTLIER_COL, MODIFIED_COL, WINDOW_TYPE_COL,
                     SUBCLASS_COL]


_CLS = (
    # this is the class names definition. Each element is
    # 1. the class name (string) WHICH MUST BE UNIQUE
    # 2. whether it has to be considered a class mapped to 'outlier' (e.g.,
    #  set it to True if an instance of this class is correctly classified
    #  when it's classified as outlier. Set to False otherwise - i.e. inlier)
    # 3. The weight, used for calculating scores in the evaluation reports html
    ('ok', False, 100),
    ('outl. (wrong inv. file)', True, 100),
    ('outl. (cha. resp. acc <-> vel)', True, 10),
    ('outl. (gain X100 or X0.01)', True, 50),
    ('outl. (gain X10 or X0.1)', True, 5),
    ('outl. (gain X2 or X0.5)', True, 1),
    ('outlier', True, 100),
    ('unlabeled (suspicious outl.)', True, 5),
    ('unlabeled (unknown)', False, 1)
)


# test that we have unique class names:
if len(set(_[0] for _ in _CLS)) != len(_CLS):
    raise ValueError('No unique class names provided in _CLS')


def classes_of(dataframe):
    '''Returns a dictionary of class names (strings) mapped to a function:
    `f(dataframe)` which, applied to any dataframe, returns the dataframe
    filtered with only rows of that class

    The classes of each dataframe are dataset dependent.

    For usages of this method, see `sod.core.plot` or
    `sod.core.evaluation.cmatrix_df`
    '''
    if MODIFIED_COL in dataframe.columns:  # e.g., oneminutewindows.hdf
        return odict([
            (_CLS[0][0], lambda dfr:~is_outlier(dfr)),
            (_CLS[1][0], is_out_wrong_inv),
            (_CLS[2][0], is_out_swap_acc_vel),
            (_CLS[3][0], is_out_gain_x100),
            (_CLS[4][0], is_out_gain_x10),
            (_CLS[5][0], is_out_gain_x2)
        ])

    if SUBCLASS_COL in dataframe.columns:  # e.g. magnitudeenergy.hdf

        return odict([
            (_CLS[0][0], lambda dfr:~is_outlier(dfr) & is_subclass_empty(dfr)),
            (_CLS[6][0], lambda dfr: is_outlier(dfr) & is_subclass_empty(dfr)),
            (_CLS[7][0], is_subclass_suspicious_outlier),
            (_CLS[8][0], is_subclass_unlabeled)
        ])

    raise ValueError('Dataset classes seem not to be defined')


_CLSW = {_[0]: _[2] for _ in _CLS}


def class_weight(classname):
    '''Returns the class weight for the given class name'''
    try:
        return _CLSW[classname]
    except KeyError:
        raise ValueError('class "%s" not found' % classname)


_CLSO = {_[0]: _[1] for _ in _CLS}


def is_class_outlier(classname):
    '''Returns the class weight for the given class name'''
    try:
        return _CLSO[classname]
    except KeyError:
        raise ValueError('class "%s" not found' % classname)


def is_outlier(dataframe):
    '''pandas series of boolean telling where dataframe rows are outliers'''
    return dataframe['outlier']  # simply return the column


def is_out_wrong_inv(dataframe):
    '''pandas series of boolean telling where dataframe rows are outliers
    due to wrong inventory
    '''
    return dataframe[MODIFIED_COL].str.contains('INVFILE:')


def is_out_swap_acc_vel(dataframe):
    '''pandas series of boolean telling where dataframe rows are outliers
    generated by swapping the accelerometer and velocimeter
    response in the inventory (when the segment's inventory has both
    accelerometers and velocimeters)
    '''
    return dataframe[MODIFIED_COL].str.contains('CHARESP:')


def is_out_gain_x10(dataframe):
    '''pandas series of boolean telling where dataframe rows are outliers
    generated by multiplying the trace by a factor of 100 (or 0.01)
    '''
    return dataframe[MODIFIED_COL].str.contains('STAGEGAIN:X10.0') | \
        dataframe[MODIFIED_COL].str.contains('STAGEGAIN:X0.1')


def is_out_gain_x100(dataframe):
    '''pandas series of boolean telling where dataframe rows are outliers
    generated by multiplying the trace by a factor of 10 (or 0.1)
    '''
    return dataframe[MODIFIED_COL].str.contains('STAGEGAIN:X100.0') | \
        dataframe[MODIFIED_COL].str.contains('STAGEGAIN:X0.01')


def is_out_gain_x2(dataframe):
    '''pandas series of boolean telling where dataframe rows are outliers
    generated by multiplying the trace by a factor of 2 (or 0.5)
    '''
    return dataframe[MODIFIED_COL].str.contains('STAGEGAIN:X2.0') | \
        dataframe[MODIFIED_COL].str.contains('STAGEGAIN:X0.5')


def is_subclass_suspicious_outlier(dataframe):
    return dataframe[SUBCLASS_COL].str.contains('unlabeled.maybe.outlier')


def is_subclass_unlabeled(dataframe):
    return dataframe[SUBCLASS_COL].str.contains('unlabeled.unknown')


def is_subclass_empty(dfr):
    return dfr[SUBCLASS_COL].str.len() < 1

##########################
# dataset IO function(s)
##########################


def open_dataset(filename, normalize=True, verbose=True):

    filepath = dataset_path(filename)
    keyname = splitext(basename(filepath))[0]

    try:
        func = globals()[keyname]
    except KeyError:
        raise ValueError('Invalid dataset, no function "%s" '
                         'implemented' % keyname)

    if verbose:
        print('Opening %s' % abspath(filepath))

    # capture warnings which are redirected to stderr:
    with capture_stderr(verbose):
        dfr = pd.read_hdf(filepath)

        if 'Segment.db.id' in dfr.columns:
            if ID_COL in dfr.columns:
                raise ValueError('The data frame already contains a '
                                 'column named "%s"' % ID_COL)
            # if it's a prediction dataframe, it's for backward compatib.
            dfr.rename(columns={"Segment.db.id": ID_COL}, inplace=True)

        try:
            dfr = func(dfr)
        except Exception as exc:
            raise ValueError('Check module function "%s", error: %s' % 
                             (func.__name__, str(exc)))

        if verbose:
            print('')
            print(dfinfo(dfr))

        if normalize:
            print('')
            dfr = dfnormalize(dfr, None, verbose)

    # for safety:
    dfr.reset_index(drop=True, inplace=True)
    return dfr


def dataset_path(filename, assure_exist=True):
    keyname, ext = splitext(filename)
    if not ext:
        filename += '.hdf'
    filepath = abspath(join(DATASETS_DIR, filename))
    if assure_exist and not isfile(filepath):
        raise ValueError('Invalid dataset, File not found: "%s"'
                         % filepath)
    return filepath

#################
# Functions mapped to specific datasets in 'datasets' and performing
# custom dataframe operations
#################


def pgapgv(dataframe):
    '''Custom operations to be performed on the pgapgv dataset
    (sod/datasets/pgapgv.hdf)
    '''
    # setting up columns:
    dataframe['pga'] = np.log10(dataframe['pga_observed'].abs())
    dataframe['pgv'] = np.log10(dataframe['pgv_observed'].abs())
    dataframe['delta_pga'] = np.log10(dataframe['pga_observed'].abs()) - \
        np.log10(dataframe['pga_predicted'].abs())
    dataframe['delta_pgv'] = np.log10(dataframe['pgv_observed'].abs()) - \
        np.log10(dataframe['pgv_predicted'].abs())
    del dataframe['pga_observed']
    del dataframe['pga_predicted']
    del dataframe['pgv_observed']
    del dataframe['pgv_predicted']
    for col in dataframe.columns:
        if col.startswith('amp@'):
            # go to db. We should multuply log * 20 (amp spec) or * 10 (pow
            # spec) but it's unnecessary as we will normalize few lines below
            dataframe[col] = np.log10(dataframe[col])
    # save space:
    dataframe[MODIFIED_COL] = dataframe[MODIFIED_COL].astype('category')
    # numpy int64 for just zeros and ones is waste of space: use bools
    # (int8). But first, let's be paranoid first (check later, see below)
    _zum = dataframe[OUTLIER_COL].sum()
    # convert:
    dataframe[OUTLIER_COL] = dataframe[OUTLIER_COL].astype(bool)
    # check:
    if dataframe[OUTLIER_COL].sum() != _zum:
        raise ValueError('The column "outlier" is supposed to be '
                         'populated with zeros or ones, but conversion '
                         'to boolean failed. Check the column')
    return dataframe


def oneminutewindows(dataframe):
    '''Custom operations to be performed on the oneminutewindows dataset
    (sod/datasets/oneminutewindows.hdf)
    '''
    # save space:
    dataframe[MODIFIED_COL] = dataframe[MODIFIED_COL].astype('category')
    dataframe['window_type'] = dataframe['window_type'].astype('category')
    return dataframe

# def magnitudeenergy_old(dataframe):
#     '''Custom operations to be performed on the magnitudeenergy_old dataset
#     (sod/datasets/magnitudeenergy_old.hdf)
#     '''
#     # set the outlier where suspect is True as True:
#     dataframe.loc[is_unlabeled_subclass_suspicious_outlier(dataframe),
#                   OUTLIER_COL] = True
#     return dataframe


def magnitudeenergy(dataframe):
    '''Custom operations to be performed on the magnitudeenergy dataset
    (sod/datasets/magnitudeenergy.hdf)
    '''
    # save space:
    dataframe[SUBCLASS_COL] = dataframe[SUBCLASS_COL].astype('category')
    # set the outlier where suspect is True as True:
    dataframe.loc[is_subclass_suspicious_outlier(dataframe),
                  OUTLIER_COL] = True
    # dataframe['modified'] = dataframe['modified'].astype('category')
    return dataframe

###########################
# Other operations
###########################


@contextmanager
def capture_stderr(verbose=False):
    '''Context manager to be used in a with statement in order to capture
    std.error messages (e.g., python warnings):
    ```
    with capture_stderr():
        ... code here
    ```
    :param verbose: boolean (default False). If True, prints the captured
        messages (if present)
    '''
    # Code to acquire resource, e.g.:
    # capture warnings which are redirected to stderr:
    syserr = sys.stderr
    if isinstance(syserr, StringIO):
        # already within a captured_stderr with statement?
        yield
    else:
        captured_err = StringIO()
        sys.stderr = captured_err
        try:
            yield
            if verbose:
                errs = captured_err.getvalue()
                if errs:
                    print('')
                    print('During the operation, '
                          'the following warning(s) were issued:')
                    print(errs)
            captured_err.close()
        finally:
            # restore standard error:
            sys.stderr = syserr


def dfinfo(dataframe, perclass=True):
    '''Returns a adataframe with info about the given `dataframe` representing
    a given dataset
    '''
    columns = ['instances']
    if not perclass:
        oks = ~is_outlier(dataframe)
        oks_count = oks.sum()
        data = [oks_count, len(dataframe) - oks_count, len(dataframe)]
        index = ['oks', 'outliers', 'total']
    else:
        classes = classes_of(dataframe)
        data = [_(dataframe).sum() for _ in classes.values()] + \
            [len(dataframe)]
        index = list(classes.keys()) + ['total']

    return df2str(pd.DataFrame(data, columns=columns, index=index))


def df2str(dataframe):
    ''':return: the string representation of `dataframe`, with numeric values
    formatted with comma as decimal separator
    '''
    return _dfformat(dataframe).to_string()


def _dfformat(dataframe, n_decimals=2):
    '''Returns a copy of `dataframe` with all numeric values converted to
    formatted strings (with comma as thousand separator)

    :param n_decimals: how many decimals to display for floats (defautls to 2)
    '''
    float_frmt = '{:,.' + str(n_decimals) + 'f}'  # e.g.: '{:,.2f}'
    strformat = {
        c: "{:,d}" if str(dataframe[c].dtype).startswith('int') else float_frmt
        for c in dataframe.columns
    }
    return pd.DataFrame({c: dataframe[c].map(strformat[c].format)
                         for c in dataframe.columns},
                        index=dataframe.index)


def dfnormalize(dataframe, columns=None, verbose=True):
    '''Normalizes dataframe under the sepcified columns. Only good instances
    (not outliers) will be considered in the normalization

    :param columns: if None (the default), nornmalizes on floating columns
        only. Otherwise, it is a list of strings denoting the columns on
        which to normalize
    '''
    sum_df = {}
    if verbose:
        if columns is None:
            print('Normalizing numeric columns (floats only)')
        else:
            print('Normalizing %s' % str(columns))
        print('(only good instances - no outliers - taken into account)')

    with capture_stderr(verbose):
        infocols = ['prenorm_min', 'prenorm_max',
                    'min', 'median', 'max', 'NAs', 'ids outside [1-99]%']
        oks_ = ~is_outlier(dataframe)
        itercols = floatingcols(dataframe) if columns is None else columns
        for col in itercols:
            _dfr = dataframe.loc[oks_, :]
            q01 = np.nanquantile(_dfr[col], 0.01)
            q99 = np.nanquantile(_dfr[col], 0.99)
            df1, df99 = _dfr[(_dfr[col] < q01)], _dfr[(_dfr[col] > q99)]
            segs1 = len(pd.unique(df1[ID_COL]))
            segs99 = len(pd.unique(df99[ID_COL]))
            # stas1 = len(pd.unique(df1['station_id']))
            # stas99 = len(pd.unique(df99['station_id']))

            # for calculating min and max, we need to drop also infinity, tgus
            # np.nanmin and np.nanmax do not work. Hence:
            finite_values = _dfr[col][np.isfinite(_dfr[col])]
            min_, max_ = np.min(finite_values), np.max(finite_values)
            dataframe[col] = (dataframe[col] - min_) / (max_ - min_)
            if verbose:
                sum_df[col] = {
                    infocols[0]: min_,
                    infocols[1]: max_,
                    infocols[2]: dataframe[col].min(),
                    infocols[3]: dataframe[col].quantile(0.5),
                    infocols[4]: dataframe[col].max(),
                    infocols[5]: (~np.isfinite(dataframe[col])).sum(),
                    infocols[6]: segs1 + segs99,
                    # columns[5]: stas1 + stas99,
                }
        if verbose:
            print(df2str(pd.DataFrame(data=list(sum_df.values()),
                                      columns=infocols,
                                      index=list(sum_df.keys()))))
            print("-------")
            print("Min and max might be outside [0, 1]: the normalization ")
            print("bounds are calculated on good segments (non outlier) only")
            print("%s: values which are NaN or Infinity" % infocols[5])
            print("%s: good instances (not outliers) the given percentiles" % 
                  infocols[6])

    return dataframe


def floatingcols(dataframe):
    '''Iterable yielding all floating point columns of dataframe'''
    for col in dataframe.columns:
        try:
            if np.issubdtype(dataframe[col].dtype, np.floating):
                yield col
        except TypeError:
            # categorical data falls here
            continue


####################
# TO BE TESTED!!!!
####################


NUM_SEGMENTS_COL = 'num_segments'


def is_station_df(dataframe):
    '''Returns whether the given dataframe is the result of `groupby_station`
    on a given segment-based dataframe
    '''
    return NUM_SEGMENTS_COL in dataframe.columns


def groupby_stations(dataframe, verbose=True):
    '''Groups `dataframe` by stations and returns the resulting dataframe
    Numeric columns are merged taking the median of all rows
    '''
    if verbose:
        print('Grouping dataset per station')
        print('(For floating columns, the median of all segments stations '
              'will be set)')
        print('')
    with capture_stderr(verbose):
        newdf = []
        fl_cols = list(floatingcols(dataframe))
        for (staid, modified, outlier), _df in \
                dataframe.groupby(['station_id', 'modified', 'outlier']):
            _dfmedian = _df[fl_cols].median(axis=0, numeric_only=True,
                                            skipna=True)
            _dfmedian[NUM_SEGMENTS_COL] = len(_df)
            _dfmedian['outlier'] = outlier
            _dfmedian['modified'] = modified
            _dfmedian[ID_COL] = staid
            newdf.append(pd.DataFrame([_dfmedian]))
            # print(pd.DataFrame([_dfmedian]))

        ret = pdconcat(newdf, ignore_index=True)
        ret[NUM_SEGMENTS_COL] = ret[NUM_SEGMENTS_COL].astype(int)
        # convert dtypes because they might not match:
        shared_c = (set(dataframe.columns) & set(ret.columns)) - set(fl_cols)
        for col in shared_c:
            ret[col] = ret[col].astype(dataframe[col].dtype)
        if verbose:
            bins = [1, 10, 100, 1000, 10000]
            max_num_segs = ret[NUM_SEGMENTS_COL].max()
            if max_num_segs >= 10 * bins[-1]:
                bins.append(max_num_segs + 1)
            elif max_num_segs >= bins[-1]:
                bins[-1] = max_num_segs + 1
            groups = ret.groupby(pd.cut(ret[NUM_SEGMENTS_COL], bins,
                                        precision=0,
                                        right=False))
            print(pd.DataFrame(groups.size(), columns=['num_stations']).
                  to_string())
            assert groups.size().sum() == len(ret)
            print('')
            print('Summary of the new dataset (instances = stations)')
            print(dfinfo(ret))
        return ret
