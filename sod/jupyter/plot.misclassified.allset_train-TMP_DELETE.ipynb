{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading IsolationForest?features=psd@5sec&contamination=0.1&max_samples=2048&n_estimators=100&behaviour=new\n",
      "-0.48911202968027523\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fe5b4e6e41e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mpred_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_hdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaindir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'predictions'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.hdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# setting stuff:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "import importlib, yaml\n",
    "import os, pandas as pd, numpy as np\n",
    "from joblib import dump, load\n",
    "import matplotlib.pyplot as plt\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "import sod.core.dataset as sod_core_dataset\n",
    "importlib.reload(sod_core_dataset)\n",
    "import sod.core.plot as sod_plot\n",
    "importlib.reload(sod_plot)\n",
    "from sod.core import pdconcat\n",
    "# from sod.core.dataset import open_dataset # , normalize_df\n",
    "# from sod import plot\n",
    "\n",
    "is_outlier = sod_core_dataset.is_outlier\n",
    "\n",
    "maindir = os.path.abspath(os.path.join(os.getcwd(), '..', 'evaluations/results/cv.allset_train.iforest.yaml/'))\n",
    "assert os.path.isdir(maindir)\n",
    "\n",
    "name = 'IsolationForest?features=psd@5sec&contamination=0.1&max_samples=2048&n_estimators=100&behaviour=new'\n",
    "print('Reading %s' % name)\n",
    "clf = load(os.path.join(maindir, 'models', name+'.model'))\n",
    "\n",
    "print(clf.offset_)\n",
    "raise\n",
    "pred_df = pd.read_hdf(os.path.join(maindir, 'predictions', name+'.hdf'))\n",
    "# setting stuff:\n",
    "pred_df['channel_code'] = pred_df['channel_code'].astype('category')\n",
    "pred_df['location_code'] = pred_df['location_code'].astype('category')\n",
    "pred_df['cha_id'] = pred_df['channel_code'].str[:2]\n",
    "pred_df['cha_id'] = pred_df['cha_id'].astype('category')\n",
    "pred_df['score_samples'] = -(pred_df['decision_function'] + clf.offset_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "classnames = sod_core_dataset.allset_train.classnames[1:]\n",
    "classes = {_: sod_core_dataset.allset_train.class_selector[_] for _ in classnames}\n",
    "\n",
    "# def select(df, classes):\n",
    "#     sel = None\n",
    "#     for k, v in classes.items():\n",
    "#         if sel is None:\n",
    "#             sel = v(df)\n",
    "#         else:\n",
    "#             sel |= v(df)\n",
    "#     return df[sel]\n",
    "\n",
    "#_pred_df = select(pred_df, classes)\n",
    "_pred_df = pred_df\n",
    "print('Reading source train dataframe for event times')\n",
    "dfr = pd.read_hdf('/Users/riccardo/work/gfz/projects/sources/python/sod/sod/datasets/allset_train.hdf',\n",
    "           columns=['Segment.db.id', 'dataset_id', 'event_time'])\n",
    "dfr.rename({'Segment.db.id': 'allset_train.id'}, axis=1, inplace=True)\n",
    "assert 'event_time' not in _pred_df.columns\n",
    "_pred_df = _pred_df.merge(dfr, how='left', on=['allset_train.id', 'dataset_id'])\n",
    "assert 'event_time' in _pred_df.columns\n",
    "\n",
    "tmin, tmax = pd.to_datetime(_pred_df.event_time.min()), pd.to_datetime(_pred_df.event_time.max())\n",
    "\n",
    "#print('%d segments of %d misclassified with score diff >=%f' % (len(_pred_df), len(pred_df), TH))\n",
    "\n",
    "_fle = os.path.abspath(os.path.join('.', 'jnconfig.yaml'))\n",
    "assert os.path.isfile(_fle)\n",
    "with open(_fle, \"r\") as _:\n",
    "    jnconfig = yaml.safe_load(_)\n",
    "\n",
    "dataset_urls= {\n",
    "    1: jnconfig['dbpath_new'],\n",
    "    2: jnconfig['dbpath_me'],\n",
    "    3: jnconfig['dbpath_chile']\n",
    "}\n",
    "\n",
    "\n",
    "station_codes = {}\n",
    "\n",
    "from stream2segment.process.db import get_session\n",
    "from stream2segment.io.db.models import Station, Segment, concat, Channel\n",
    "from stream2segment.io.utils import loads_inv\n",
    "\n",
    "print('Getting station networks and stations')\n",
    "for dts_id, df in _pred_df.groupby('dataset_id'):\n",
    "    sess = get_session(dataset_urls[dts_id])\n",
    "    for (staid, stanet, stasta) in sess.query(Station.id, Station.network, Station.station):\n",
    "        station_codes[(dts_id, staid)] = \"%s.%s\" % (stanet, stasta)\n",
    "    sess.close()\n",
    "\n",
    "        \n",
    "columns = ['dataset_id', 'station_id', 'cha_id', 'location_code']\n",
    "# _group = _pred_df.groupby(columns)\n",
    "# print('%d distinct channels found, %d channels (no orientation)' % (len(_group), len(_group)/3))\n",
    "\n",
    "\n",
    "spreds = []\n",
    "for cname, csel in classes.items():\n",
    "    class_df = _pred_df[csel(_pred_df)]\n",
    "    for (dts_id, sta_id, cha_id, loc_code), df in class_df.groupby(columns):\n",
    "        key = '%s.%s.%s?' % (station_codes[(dts_id, sta_id)], loc_code, cha_id)\n",
    "        key += ' (database: %s, station id: %d)' % (dataset_urls[dts_id][dataset_urls[dts_id].rfind('/')+1:], sta_id)\n",
    "        med = df.score_samples.median()\n",
    "        outl_count = df.outlier.sum()\n",
    "        if outl_count == len(df) and med < 0.5:\n",
    "            spreds.append([key, True, cname, df.event_time, df.score_samples])\n",
    "        elif outl_count == 0 and med > 0.5:\n",
    "            spreds.append([key, False, cname, df.event_time, df.score_samples])\n",
    "        elif outl_count != len(df) and outl_count !=0:\n",
    "            print('%d: ??' % (sta_id))\n",
    "\n",
    "@contextmanager\n",
    "def plotparams(params):\n",
    "    '''makes temporarily matplotlib params.\n",
    "    Make sure to run this after %matplotlib inline.\n",
    "    For info see https://stackoverflow.com/questions/36367986/how-to-make-inline-plots-in-jupyter-notebook-larger'''\n",
    "    def_params = {k: plt.rcParams[k] for k in params}\n",
    "    for k, v in params.items():\n",
    "        plt.rcParams[k] = v\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        for k, v in def_params.items():\n",
    "            plt.rcParams[k] = v\n",
    "\n",
    "            \n",
    "spreds.sort(key=lambda val: np.nanmedian(val[-1]), reverse=True)\n",
    "assert any('wrong inv' in _[2] for _ in spreds)\n",
    "\n",
    "print()\n",
    "print()\n",
    "title = '%d channels to check (one plot per channel, each blue point is one event\\'s score):' % len(spreds)\n",
    "print(title)\n",
    "print('=' * len(title))\n",
    "print()\n",
    "with plotparams({'figure.figsize': (50, 10), 'font.size': 32}):\n",
    "    for i, (key, outlier, cname, times, vals) in enumerate(spreds):\n",
    "        median_score = vals.median()\n",
    "        plt.axhline(median_score, color='r', lw=5, linestyle='dotted')\n",
    "        plt.scatter([pd.to_datetime(_) for _ in times], vals, s=121)\n",
    "        plt.xlim([tmin, tmax])\n",
    "        comment = \"Channel \" + key\n",
    "        comment += '. Median score %.3f (red line), label \"%s\"' % (median_score, cname)\n",
    "        # comment += ('<0.5 (label: \"%s\")' if not outlier else '>0.5 (label: \"%s\")') % cname\n",
    "        print(comment)  # printing makes it easy to search within the web browser\n",
    "        # plt.title(key + '\\n' + comment)\n",
    "        plt.ylabel('scores (0: ok, 1:anomaly)')\n",
    "        plt.xlabel('event time')\n",
    "        plt.show()\n",
    "#         if i > 1:\n",
    "#             break\n",
    "\n",
    "\n",
    "# path = os.path.join(evalreports, 'evaluations.all.hdf')\n",
    "\n",
    "# dfr = pd.read_hdf(path)\n",
    "# print('evaulation dataframe.\\n\\nColumns:\\n' + str(dfr.columns.tolist()))\n",
    "# print('\\nclassname distinct values:\\n' + str(np.unique(dfr['classname'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
